#!/usr/bin/env python

import argparse
import os
import re
from importlib.resources import files

import pandas as pd

from pipelines import PIPELINE_CATALOG
from talent_tools.findTAL import RunFindTALTask
from talent_tools.talutil import OptionObject

from .logging import logger

# MAGIC NUMBERS for the TALE-NT Tool
MIN_SPACER = 14
MAX_SPACER = 18
ARR_MIN = 14
ARR_MAX = 18
FILTER = 1  # keep 1 for specific efficient TALES ELSE keep 2 to generate all kinds of TALE! (see TALEN FAQ page)
CUT_POS = 31


def main():
    parser = argparse.ArgumentParser(description='Process DNA sequence for base editing.')
    # yapf: disable
    parser.add_argument('--mtdna_seq_path', '-i', type=str, default=None, help='File containing the mtDNA sequence as plain text.')
    parser.add_argument('--bystander_file'      , type=str,                     help='Excel file containing bystander effect annotations (optional, for human mtDNA analysis)')
    parser.add_argument('--output_prefix'       , type=str, default='mitoedit', help='Prefix for output CSV files (default: mitoedit)')
    parser.add_argument('--min_spacer'          , type=int, default=MIN_SPACER, help=f'Minimum spacer length for TALE-NT (default: {MIN_SPACER})')
    parser.add_argument('--max_spacer'          , type=int, default=MAX_SPACER, help=f'Maximum spacer length for TALE-NT (default: {MAX_SPACER})')
    parser.add_argument('--array_min'           , type=int, default=ARR_MIN,    help=f'Minimum array length for TALE-NT (default: {ARR_MIN})')
    parser.add_argument('--array_max'           , type=int, default=ARR_MAX,    help=f'Maximum array length for TALE-NT (default: {ARR_MAX})')
    parser.add_argument('--filter'              , type=int, default=FILTER,     help=f'TALE-NT filter setting (default: {FILTER})')
    parser.add_argument('--cut_pos'             , type=int, default=CUT_POS,    help=f'TALE-NT cut position (default: {CUT_POS})')
    parser.add_argument('position'              , type=int,                     help='Position of the base to be changed')
    parser.add_argument('mutant_base'           , type=str,                     help='Mutant base to be changed into')
    # yapf: enable
    args = parser.parse_args()

    # Convert mutant base to uppercase
    mutant_base = args.mutant_base.upper()

    # Read mtDNA sequence from text file
    logger.info("Reading mtDNA sequence from file: %s", args.mtdna_seq_path)
    with open(args.mtdna_seq_path, "r") as fh:
        mtdna_seq = fh.read().replace("\n", "")

    # Get the reference base at the specified position
    reference_base = mtdna_seq[args.position - 1].upper()
    logger.info("Reference base at position %d is %s", args.position, reference_base)

    # Select appropriate pipelines based on base change type
    if (reference_base, mutant_base) in [('C', 'T'), ('G', 'A')]:
        pipelines = ["Mok2020_G1397", "Mok2020_G1333", "Mok2022_DddA11"]
    elif (reference_base, mutant_base) in [('A', 'G'), ('T', 'C')]:
        pipelines = ["Cho_sTALEDs"]
    else:
        raise ValueError(f"No pipeline found for reference base {reference_base} and the mutant base {mutant_base}")

    logger.info("Selected pipelines: %s", pipelines)

    windows_dfs = []
    bystanders_dfs = []

    # Run each applicable pipeline and collect results
    for pipeline in pipelines:
        logger.info("Processing pipeline: %s", pipeline)

        # Get the selected pipeline instance
        if pipeline not in PIPELINE_CATALOG:
            raise ValueError(f"Unknown pipeline: {pipeline}")
        pipeline_instance = PIPELINE_CATALOG[pipeline]()

        logger.info("Processing mtDNA sequence for position %d using pipeline %s.", args.position, pipeline)
        all_windows, adjacent_bases = pipeline_instance.process_mtDNA(mtdna_seq, args.position)

        if not adjacent_bases:
            logger.warning("The base found at position %d cannot edited by the pipeline %s.", args.position, pipeline)
            continue

        # Write adjacent bases to FASTA file
        logger.info("Writing adjacent bases to FASTA file.")
        fasta_content = pipeline_instance.list_to_fasta(adjacent_bases, args.position)
        fasta_file = f'{args.output_prefix}/adjacent_bases_{pipeline}.fasta'
        with open(fasta_file, 'w') as file:
            file.write(fasta_content)
            logger.info("Finished writing FASTA file to %s.", fasta_file)

        additional_file = args.bystander_file
        if additional_file:
            additional_file = os.path.abspath(additional_file)

        # Process pipeline data and get DataFrames
        logger.info("Processing pipeline data.")
        windows_df, bystanders_df = pipeline_instance.process_bystander_data(all_windows, additional_file)

        # Write individual pipeline CSV files
        pipeline_windows_csv = f'{args.output_prefix}/pipeline_windows_{pipeline}.csv'
        pipeline_bystanders_csv = f'{args.output_prefix}/pipeline_bystanders_{pipeline}.csv'
        
        logger.info("Writing pipeline windows data to %s.", pipeline_windows_csv)
        windows_df.to_csv(pipeline_windows_csv, index=False)
        
        if not bystanders_df.empty:
            logger.info("Writing pipeline bystanders data to %s.", pipeline_bystanders_csv)
            bystanders_df.to_csv(pipeline_bystanders_csv, index=False)
        else:
            logger.info("No bystanders information available to write.")

        # Accumulate DataFrames in lists
        windows_dfs.append(windows_df)
        if not bystanders_df.empty:
            bystanders_dfs.append(bystanders_df)

    # Combine all DataFrames at once
    all_windows_combined = pd.concat(windows_dfs, ignore_index=True) if windows_dfs else pd.DataFrame()
    all_bystanders_combined = pd.concat(bystanders_dfs, ignore_index=True) if bystanders_dfs else pd.DataFrame()

    # Save combined results from all pipelines to CSV files
    combined_windows_csv = f'{args.output_prefix}/all_windows.csv'
    combined_bystanders_csv = f'{args.output_prefix}/all_bystanders.csv'
    
    logger.info("Saving combined windows data to %s.", combined_windows_csv)
    all_windows_combined.to_csv(combined_windows_csv, index=False)
    
    if not all_bystanders_combined.empty:
        logger.info("Saving combined bystanders data to %s.", combined_bystanders_csv)
        all_bystanders_combined.to_csv(combined_bystanders_csv, index=False)
    else:
        logger.info("No combined bystanders data to save.")

    logger.info("Combined output saved successfully.")

    # Run TALE-NT analysis to find TALEN sequences
    talen_output_path = f'{args.output_prefix}/talen_output.txt'
    logger.info("Running TALE-NT findTAL analysis on %s", fasta_file)

    RunFindTALTask(
        OptionObject(
            fasta=fasta_file,
            min=args.min_spacer,
            max=args.max_spacer,
            arraymin=args.array_min,
            arraymax=args.array_max,
            outpath=talen_output_path,
            filter=args.filter,
            filterbase=args.cut_pos if args.filter == 1 else -1,
            cupstream=0,
            gspec=False,
            streubel=False,
            check_offtargets=False,
            offtargets_fasta='NA',
            offtargets_ncbi='NA',
            genome=False,
            promoterome=False,
            organism='NA',
            logFilepath='NA',
            nodeID=-1,
            ip_address='',
        ))
    logger.info("TALE-NT analysis completed successfully")

    # Load and process TALEN output to match with pipeline results
    logger.info("Loading TXT file from %s", talen_output_path)
    talen_output_df = pd.read_csv(talen_output_path, delimiter='\t', skiprows=2)
    logger.info("Successfully loaded TXT file.")

    if 'Plus strand sequence' not in talen_output_df.columns:
        raise ValueError("Column 'Plus strand sequence' not found in the TALEN file")

    plus_strand_sequences = talen_output_df['Plus strand sequence'].tolist()
    txt_bases = set(re.findall(r'[a-z]+', ' '.join(plus_strand_sequences)))

    # Match window sequences with TALEN sequences and extract left/right TALE parts
    # For each matching sequence, parse the TALEN format to separate left and right TALEs
    for index, row in all_windows_combined.iterrows():
        cleaned_sequence = re.sub(r'[{}\[\]]', '', row['Window Sequence']).lower()
        all_windows_combined.at[index, 'Matching TALEs'] = cleaned_sequence in txt_bases
        for txt_base in txt_bases:
            if txt_base == cleaned_sequence:
                left_index = 1
                right_index = 1
                matching_sequences = [
                    sequence for sequence in plus_strand_sequences if re.sub(r'[^a-z]', '', sequence) == txt_base
                ]
                for sequence in matching_sequences:
                    lower_indices = [i for i, char in enumerate(sequence) if char.islower()]
                    if not lower_indices:
                        continue
                    first_lower_index = lower_indices[0]
                    last_lower_index = lower_indices[-1]
                    L_part = sequence[:first_lower_index].strip()
                    R_part = sequence[last_lower_index + 1:].strip()
                    logger.info('index - %s, Left TALE - %s, Right TALE - %s', index, L_part, R_part)
                    all_windows_combined.at[index, f'LeftTALE{left_index}'] = L_part
                    all_windows_combined.at[index, f'RightTALE{right_index}'] = R_part
                    left_index += 1
                    right_index += 1

    logger.info("Done adding the TALE sequences!")

    matching_windows_csv = f'{args.output_prefix}/matching_tales_windows.csv'
    logger.info("Saving matching tales windows data to %s.", matching_windows_csv)
    all_windows_combined.to_csv(matching_windows_csv, index=False)
    
    # Prepare final Excel output with TALE sequences
    if args.bystander_file or args.mtdna_seq_path is None:
        bystanders_to_use = all_bystanders_combined
    else:
        bystanders_to_use = pd.DataFrame()

    if not bystanders_to_use.empty and 'Bystander Position' in bystanders_to_use.columns:
        logger.info("Filtering Bystander Effects to a maximum of three entries per Bystander Position.")
        bystanders_to_use = bystanders_to_use.groupby('Bystander Position').head(3).reset_index(drop=True)
        bystanders_to_use = bystanders_to_use.sort_values(by='Bystander Position').reset_index(drop=True)
    else:
        logger.info("No Bystander Data to process.")

    final_windows_csv = f'{args.output_prefix}/final_windows.csv'
    final_bystanders_csv = f'{args.output_prefix}/final_bystanders.csv'
    
    logger.info("Saving final windows data to %s.", final_windows_csv)
    all_windows_combined.to_csv(final_windows_csv, index=False)
    
    if not bystanders_to_use.empty:
        logger.info("Saving final bystanders data to %s.", final_bystanders_csv)
        bystanders_to_use.to_csv(final_bystanders_csv, index=False)
    else:
        logger.info("No final bystanders data to save.")

    logger.info("Final CSV files saved successfully.")


if __name__ == "__main__":
    main()
