#!/usr/bin/env python

import argparse
import os
import re
from importlib.resources import files

import pandas as pd

from pipelines import PIPELINE_CATALOG
from talent_tools.findTAL import RunFindTALTask
from talent_tools.talutil import OptionObject

from .logging import logger

# MAGIC NUMBERS for the TALE-NT Tool
MIN_SPACER = 14
MAX_SPACER = 18
ARR_MIN = 14
ARR_MAX = 18
FILTER = 1  # keep 1 for specific efficient TALES ELSE keep 2 to generate all kinds of TALE! (see TALEN FAQ page)
CUT_POS = 31



def main():
    parser = argparse.ArgumentParser(description='Process DNA sequence for base editing.')
    # yapf: disable
    parser.add_argument('--mtdna_seq_path', '-i', type=str, default=None, help='File containing the mtDNA sequence as plain text.')
    parser.add_argument('--bystander_file'      , type=str,                     help='Excel file containing bystander effect annotations (optional, for human mtDNA analysis)')
    parser.add_argument('--output', '-o'        , type=str,                     help='Excel output file path (default: final_output/final_{position}.xlsx)')
    parser.add_argument('--output_dir'          , type=str,                     help='Base output directory (default: current directory)')
    parser.add_argument('--min_spacer'          , type=int, default=MIN_SPACER, help=f'Minimum spacer length for TALE-NT (default: {MIN_SPACER})')
    parser.add_argument('--max_spacer'          , type=int, default=MAX_SPACER, help=f'Maximum spacer length for TALE-NT (default: {MAX_SPACER})')
    parser.add_argument('--array_min'           , type=int, default=ARR_MIN,    help=f'Minimum array length for TALE-NT (default: {ARR_MIN})')
    parser.add_argument('--array_max'           , type=int, default=ARR_MAX,    help=f'Maximum array length for TALE-NT (default: {ARR_MAX})')
    parser.add_argument('--filter'              , type=int, default=FILTER,     help=f'TALE-NT filter setting (default: {FILTER})')
    parser.add_argument('--cut_pos'             , type=int, default=CUT_POS,    help=f'TALE-NT cut position (default: {CUT_POS})')
    parser.add_argument('position'              , type=int,                     help='Position of the base to be changed')
    parser.add_argument('reference_base'        , type=str,                     help='Original base of the position')
    parser.add_argument('mutant_base'           , type=str,                     help='Mutant base to be changed into')
    # yapf: enable
    args = parser.parse_args()

    # Convert reference and mutant bases to uppercase
    reference_base = args.reference_base.upper()
    mutant_base = args.mutant_base.upper()

    # Read mtDNA sequence from text file
    logger.info("Reading mtDNA sequence from file: %s", args.mtdna_seq_path)
    with open(args.mtdna_seq_path, "r") as fh:
        mtdna_seq = fh.read().replace("\n", "")

    # Verify reference base matches sequence at specified position
    if reference_base != mtdna_seq[args.position - 1].upper():
        logger.error(
            f"Incorrect reference base at position {args.position}. Expected: {reference_base}, Found: {mtdna_seq[args.position - 1].upper()}"
        )
        raise ValueError(
            f"Incorrect reference base at position {args.position}. Expected: {reference_base}, Found: {mtdna_seq[args.position - 1].upper()}"
        )

    # Select appropriate pipelines based on base change type
    if (reference_base, mutant_base) in [('C', 'T'), ('G', 'A')]:
        pipelines = ["Mok2020_G1397", "Mok2020_G1333", "Mok2022_DddA11"]
        logger.info("Not editable by the Cho pipeline")
    elif (reference_base, mutant_base) in [('A', 'G'), ('T', 'C')]:
        pipelines = ["Cho_sTALEDs"]
        logger.info("Not editable by the Mok pipelines")
    else:
        logger.error(f"No pipeline found for reference base {reference_base} and the mutant base {mutant_base}")
        raise ValueError(f"No pipeline found for reference base {reference_base} and the mutant base {mutant_base}")

    all_windows_combined = pd.DataFrame()
    all_bystanders_combined = pd.DataFrame()

    # Run each applicable pipeline and collect results
    for pipeline in pipelines:
        logger.info("Processing pipeline: %s", pipeline)

        # Get the selected pipeline instance
        if pipeline not in PIPELINE_CATALOG:
            raise ValueError(f"Unknown pipeline: {pipeline}")
        pipeline_instance = PIPELINE_CATALOG[pipeline]()

        logger.info("Processing mtDNA sequence for position %d using pipeline %s.", args.position, pipeline)
        all_windows, adjacent_bases = pipeline_instance.process_mtDNA(mtdna_seq, args.position)

        if not adjacent_bases:
            logger.warning("The base found at position %d cannot edited by the pipeline %s.", args.position, pipeline)
            continue

        # Define paths for output files
        parent_directory = os.path.dirname(os.path.abspath(__file__))
        if args.output_dir:
            base_dir = os.path.abspath(args.output_dir)
        elif parent_directory:
            base_dir = parent_directory
        else:
            base_dir = os.path.dirname(os.path.abspath(__file__))
        running_directory = os.path.join(base_dir, 'running')
        os.makedirs(running_directory, exist_ok=True)

        directories = {
            'fasta': os.path.join(running_directory, 'fasta'),
            'pipeline_windows': os.path.join(running_directory, 'pipeline_windows'),
            'all_windows': os.path.join(running_directory, 'all_windows'),
            'talen': os.path.join(running_directory, 'talen'),
            'matching_output': os.path.join(running_directory, 'matching_output'),
            'final_output': os.path.join(base_dir, 'final_output')
        }

        for dir_path in directories.values():
            os.makedirs(dir_path, exist_ok=True)

        # Define output file paths
        fasta_file = os.path.join(directories['fasta'], f'adjacent_bases_{args.position}.fasta')
        allw_file = os.path.join(directories['pipeline_windows'], f'{pipeline}_{args.position}.xlsx')

        # Write adjacent bases to FASTA file
        logger.info("Writing adjacent bases to FASTA file.")
        fasta_content = pipeline_instance.list_to_fasta(adjacent_bases, args.position)
        with open(fasta_file, 'w') as file:
            file.write(fasta_content)
            logger.info("Finished writing FASTA file to %s.", fasta_file)

        additional_file = args.bystander_file
        if additional_file:
            additional_file = os.path.abspath(additional_file)

        # Process pipeline data and get DataFrames
        logger.info("Processing pipeline data.")
        windows_df, bystanders_df = pipeline_instance.process_bystander_data(all_windows, additional_file)

        # Write individual pipeline Excel file
        with pd.ExcelWriter(allw_file, engine='openpyxl', mode='w') as writer:
            windows_df.to_excel(writer, sheet_name='All_Windows', index=False)
            if not bystanders_df.empty:
                logger.info("Writing bystanders information to Excel file.")
                bystanders_df.to_excel(writer, sheet_name='Bystanders_Info', index=False)
            else:
                logger.info("No bystanders information available to write.")

        # Concatenate to combined DataFrames
        all_windows_combined = pd.concat([all_windows_combined, windows_df], ignore_index=True)
        all_bystanders_combined = pd.concat([all_bystanders_combined, bystanders_df], ignore_index=True)

    # Save combined results from all pipelines to Excel file
    output_combined_path = os.path.join(directories['all_windows'], f'all_windows_{args.position}.xlsx')
    logger.info("Saving combined outputs to %s.", output_combined_path)

    with pd.ExcelWriter(output_combined_path, engine='openpyxl') as writer:
        all_windows_combined.to_excel(writer, sheet_name='All_Windows', index=False)
        all_bystanders_combined.to_excel(writer, sheet_name='Bystander_Effects', index=False)

    logger.info("Combined output saved successfully.")

    # Run TALE-NT analysis to find TALEN sequences
    out_fn = os.path.join(directories['talen'], f'TALENT_{args.position}.txt')
    logger.info("Running TALE-NT findTAL analysis on %s", fasta_file)
    
    try:
        options = OptionObject(
            fasta=fasta_file,
            min=args.min_spacer,
            max=args.max_spacer,
            arraymin=args.array_min,
            arraymax=args.array_max,
            outpath=out_fn,
            filter=args.filter,
            filterbase=args.cut_pos if args.filter == 1 else -1,
            cupstream=0,
            gspec=False,
            streubel=False,
            check_offtargets=False,
            offtargets_fasta='NA',
            offtargets_ncbi='NA',
            genome=False,
            promoterome=False,
            organism='NA',
            logFilepath='NA',
            nodeID=-1,
            ip_address='')
        
        RunFindTALTask(options)
        logger.info("TALE-NT analysis completed successfully")
    except Exception as e:
        raise Exception(f"Error running TALE-NT analysis: {e}")

    # Load and process TALEN output to match with pipeline results
    output_excel_path = os.path.join(directories['matching_output'], f'matching_tales_{args.position}.xlsx')
    logger.info("Loading TXT file from %s", out_fn)

    try:
        txt_df = pd.read_csv(out_fn, delimiter='\t', skiprows=2)
        logger.info("Successfully loaded TXT file.")
    except pd.errors.ParserError:
        raise ValueError("Error reading the TXT file. Ensure it is tab-delimited and correctly formatted.")
    except Exception as e:
        raise ValueError(f"Failed to load TXT file {out_fn}: {e}")

    if 'Plus strand sequence' not in txt_df.columns:
        logger.error("Column 'Plus strand sequence' not found in the TALEN file.")
        raise ValueError("Column 'Plus strand sequence' not found in the TALEN file")

    plus_strand_sequences = txt_df['Plus strand sequence'].tolist()
    txt_bases = set(re.findall(r'[a-z]+', ' '.join(plus_strand_sequences)))

    logger.info("Comparing Excel file sequences with TALEN_Tool TXT file bases.")
    for index, row in all_windows_combined.iterrows():
        cleaned_sequence = row['Window Sequence'].translate(str.maketrans('', '', '{}[]'))
        all_windows_combined.at[index, 'Matching TALEs'] = cleaned_sequence.lower() in txt_bases

    logger.info("Saving updated DataFrames to output Excel file %s.", output_excel_path)

    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
        all_windows_combined.to_excel(writer, sheet_name='All_Windows', index=False)
        all_bystanders_combined.to_excel(writer, sheet_name='Bystander_Effects', index=False)

    logger.info("Updated final Excel file saved successfully.")

    # Set final output file path
    if args.output:
        excel_output = args.output if args.output.endswith('.xlsx') else args.output + '.xlsx'
    else:
        excel_output = os.path.join(directories['final_output'], f'final_{args.position}.xlsx')

    # Prepare final Excel output with TALE sequences
    if args.bystander_file or args.mtdna_seq_path is None:
        bystanders_to_use = all_bystanders_combined
    else:
        bystanders_to_use = pd.DataFrame()

    logger.info("Loading TXT file for retrieving the TALE sequences from %s", out_fn)

    try:
        txt_df = pd.read_csv(out_fn, delimiter='\t', skiprows=2)
        logger.info("Successfully loaded TXT file.")
    except Exception as e:
        raise ValueError(f"Failed to load TXT file {out_fn}: {e}")

    if 'Plus strand sequence' not in txt_df.columns:
        raise ValueError("Column 'Plus strand sequence' not found in the TALEN file")

    plus_strand_sequences = txt_df['Plus strand sequence'].tolist()
    txt_bases = set(re.findall(r'[a-z]+', ' '.join(plus_strand_sequences)))

    logger.info("Appending match information to the All_Windows DataFrame.")

    for index, row in all_windows_combined.iterrows():
        cleaned_sequence = row['Window Sequence'].translate(str.maketrans('', '', '{}[]')).lower()
        for txt_base in txt_bases:
            if txt_base == cleaned_sequence:
                left_index = 1
                right_index = 1
                matching_sequences = [
                    sequence for sequence in plus_strand_sequences if re.sub(r'[^a-z]', '', sequence) == txt_base
                ]
                for sequence in matching_sequences:
                    lower_indices = [i for i, char in enumerate(sequence) if char.islower()]
                    if not lower_indices:
                        continue
                    first_lower_index = lower_indices[0]
                    last_lower_index = lower_indices[-1]
                    L_part = sequence[:first_lower_index].strip()
                    R_part = sequence[last_lower_index + 1:].strip()
                    logger.info('index - %s, Left TALE - %s, Right TALE - %s', index, L_part, R_part)
                    all_windows_combined.at[index, f'LeftTALE{left_index}'] = L_part
                    all_windows_combined.at[index, f'RightTALE{right_index}'] = R_part
                    left_index += 1
                    right_index += 1

    logger.info("Done adding the TALE sequences!")

    if not bystanders_to_use.empty and 'Bystander Position' in bystanders_to_use.columns:
        logger.info("Filtering Bystander Effects to a maximum of three entries per Bystander Position.")
        bystanders_to_use = bystanders_to_use.groupby('Bystander Position').head(3).reset_index(drop=True)
        bystanders_to_use = bystanders_to_use.sort_values(by='Bystander Position').reset_index(drop=True)
    else:
        logger.info("No Bystander Data to process.")

    logger.info("Saving updated DataFrames to output Excel file %s.", excel_output)
    with pd.ExcelWriter(excel_output, engine='openpyxl') as writer:
        all_windows_combined.to_excel(writer, sheet_name='All_Windows', index=False)
        if not bystanders_to_use.empty:
            bystanders_to_use.to_excel(writer, sheet_name='Bystander_Effects', index=False)

    logger.info("Updated final Excel file saved successfully.")


if __name__ == "__main__":
    main()
