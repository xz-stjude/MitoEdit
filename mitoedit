#!/usr/bin/env python

import argparse
import os
import re
import tempfile
from importlib.resources import files

import pandas as pd

from pipelines.Cho_sTALEDs import ChosTALEDsPipeline
from pipelines.Mok2020_unified import Mok2020UnifiedPipeline
from talent_tools.findTAL import RunFindTALTask
from talent_tools.talutil import OptionObject

import logging
import sys

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', stream=sys.stdout)

logger = logging.getLogger('mitoedit')

# MAGIC NUMBERS for the TALE-NT Tool
MIN_SPACER = 14
MAX_SPACER = 18
ARR_MIN = 14
ARR_MAX = 18
FILTER = 1  # keep 1 for specific efficient TALES ELSE keep 2 to generate all kinds of TALE! (see TALEN FAQ page)
CUT_POS = 31


def process_mitoedit(mtdna_seq, position, mutant_base, bystander_df=None, tale_nt_params=None):
    """
    Core MitoEdit processing function for programmatic use.
    
    Args:
        mtdna_seq (str): mtDNA sequence string
        position (int): Position of the base to be changed (1-based)
        mutant_base (str): Mutant base to be changed into
        bystander_df (pd.DataFrame, optional): DataFrame containing bystander effect annotations
        tale_nt_params (dict, optional): TALE-NT parameters for findTAL analysis
        
    Returns:
        dict: Results containing windows_df, bystanders_df, adjacent_bases, fasta_content, and talen_output_df
    """
    # Convert mutant base to uppercase
    mutant_base = mutant_base.upper()

    # Get the reference base at the specified position
    reference_base = mtdna_seq[position - 1].upper()
    logger.info(f"Reference base at position {position} is {reference_base}")

    # Select appropriate pipeline based on base change type
    if (reference_base, mutant_base) in [('C', 'T'), ('G', 'A')]:
        pipeline_class = Mok2020UnifiedPipeline
        pipeline_name = "Mok2020_Unified"
    elif (reference_base, mutant_base) in [('A', 'G'), ('T', 'C')]:
        pipeline_class = ChosTALEDsPipeline
        pipeline_name = "Cho_sTALEDs"
    else:
        raise ValueError(f"No pipeline found for reference base {reference_base} and the mutant base {mutant_base}")

    logger.info(f"Selected pipeline: {pipeline_name}")

    # Create the selected pipeline instance
    pipeline_instance = pipeline_class()

    logger.info(f"Processing mtDNA sequence for position {position}.")
    all_windows, adjacent_bases = pipeline_instance.process_mtDNA(mtdna_seq, position)

    if not adjacent_bases:
        logger.warning(f"The base found at position {position} cannot be edited.")
        return {
            'windows_df': pd.DataFrame(),
            'bystanders_df': pd.DataFrame(),
            'adjacent_bases': '',
            'fasta_content': '',
            'talen_output_df': pd.DataFrame()
        }

    # Convert all_windows to DataFrame
    windows_df = pd.DataFrame(all_windows,
                              columns=[
                                  'Pipeline', 'Position', 'Reference Base', 'Mutant Base', 'Window Size',
                                  'Window Sequence', 'Target Location', 'Number of Bystanders',
                                  'Position of Bystanders', 'Optimal Flanking TALEs', 'Flag (CheckBystanderEffect)'
                              ])

    # Create FASTA content
    fasta_content = f">Adjacent_bases_position_{position}\n{adjacent_bases}\n"

    # Process pipeline data and get DataFrames
    logger.info("Processing pipeline data.")
    windows_df, bystanders_df = pipeline_instance.process_bystander_data(windows_df, bystander_df)

    logger.info("Pipeline processing completed successfully.")

    # Run TALE-NT analysis to find TALEN sequences
    if tale_nt_params is None:
        tale_nt_params = {
            'min_spacer': MIN_SPACER,
            'max_spacer': MAX_SPACER,
            'array_min': ARR_MIN,
            'array_max': ARR_MAX,
            'filter': FILTER,
            'cut_pos': CUT_POS
        }

    # Create temporary FASTA file for TALE-NT analysis
    with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as temp_fasta:
        temp_fasta.write(fasta_content)
        temp_fasta_path = temp_fasta.name

    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as temp_output:
        temp_output_path = temp_output.name

    try:
        logger.info("Running TALE-NT findTAL analysis")
        RunFindTALTask(
            OptionObject(
                fasta=temp_fasta_path,
                min=tale_nt_params['min_spacer'],
                max=tale_nt_params['max_spacer'],
                arraymin=tale_nt_params['array_min'],
                arraymax=tale_nt_params['array_max'],
                outpath=temp_output_path,
                filter=tale_nt_params['filter'],
                filterbase=tale_nt_params['cut_pos'] if tale_nt_params['filter'] == 1 else -1,
                cupstream=0,
                gspec=False,
                streubel=False,
                check_offtargets=False,
                offtargets_fasta='NA',
                offtargets_ncbi='NA',
                genome=False,
                promoterome=False,
                organism='NA',
                logFilepath='NA',
                nodeID=-1,
                ip_address='',
            ))
        logger.info("TALE-NT analysis completed successfully")

        # Load and process TALEN output to match with pipeline results
        logger.info("Loading TALE-NT output")
        talen_output_df = pd.read_csv(temp_output_path, delimiter='\t', skiprows=2)
        logger.info("Successfully loaded TALE-NT output.")

        if 'Plus strand sequence' not in talen_output_df.columns:
            logger.warning("Column 'Plus strand sequence' not found in the TALEN file")
            talen_output_df = pd.DataFrame()
        else:
            plus_strand_sequences = talen_output_df['Plus strand sequence'].tolist()
            txt_bases = set(re.findall(r'[a-z]+', ' '.join(plus_strand_sequences)))

            # Match window sequences with TALEN sequences and extract left/right TALE parts
            for index, row in windows_df.iterrows():
                cleaned_sequence = re.sub(r'[{}\[\]]', '', row['Window Sequence']).lower()
                windows_df.at[index, 'Matching TALEs'] = cleaned_sequence in txt_bases
                for txt_base in txt_bases:
                    if txt_base == cleaned_sequence:
                        left_index = 1
                        right_index = 1
                        matching_sequences = [
                            sequence for sequence in plus_strand_sequences
                            if re.sub(r'[^a-z]', '', sequence) == txt_base
                        ]
                        for sequence in matching_sequences:
                            lower_indices = [i for i, char in enumerate(sequence) if char.islower()]
                            if len(lower_indices) >= 2:
                                spacer_start = lower_indices[0]
                                spacer_end = lower_indices[-1]
                                left_tale = sequence[:spacer_start].upper()
                                right_tale = sequence[spacer_end + 1:].upper()
                                windows_df.at[index, f'Left TALE {left_index}'] = left_tale
                                windows_df.at[index, f'Right TALE {right_index}'] = right_tale
                                left_index += 1
                                right_index += 1

    finally:
        # Clean up temporary files
        os.unlink(temp_fasta_path)
        os.unlink(temp_output_path)

    logger.info("All processing completed successfully.")

    return {
        'windows_df': windows_df,
        'bystanders_df': bystanders_df,
        'adjacent_bases': adjacent_bases,
        'fasta_content': fasta_content,
        'talen_output_df': talen_output_df
    }


def main():
    """CLI entry point for MitoEdit."""
    parser = argparse.ArgumentParser(description='Process DNA sequence for base editing.')
    # yapf: disable
    parser.add_argument('--mtdna_seq_path', '-i', type=str, default=None,       help='File containing the mtDNA sequence as plain text.')
    parser.add_argument('--bystander_file'      , type=str,                     help='Excel file containing bystander effect annotations (optional, for human mtDNA analysis)')
    parser.add_argument('--output_prefix'       , type=str, default='output',   help='Prefix for output CSV files (default: output)')
    parser.add_argument('--min_spacer'          , type=int, default=MIN_SPACER, help=f'Minimum spacer length for TALE-NT (default: {MIN_SPACER})')
    parser.add_argument('--max_spacer'          , type=int, default=MAX_SPACER, help=f'Maximum spacer length for TALE-NT (default: {MAX_SPACER})')
    parser.add_argument('--array_min'           , type=int, default=ARR_MIN,    help=f'Minimum array length for TALE-NT (default: {ARR_MIN})')
    parser.add_argument('--array_max'           , type=int, default=ARR_MAX,    help=f'Maximum array length for TALE-NT (default: {ARR_MAX})')
    parser.add_argument('--filter'              , type=int, default=FILTER,     help=f'TALE-NT filter setting (default: {FILTER})')
    parser.add_argument('--cut_pos'             , type=int, default=CUT_POS,    help=f'TALE-NT cut position (default: {CUT_POS})')
    parser.add_argument('position'              , type=int,                     help='Position of the base to be changed')
    parser.add_argument('mutant_base'           , type=str,                     help='Mutant base to be changed into')
    # yapf: enable
    args = parser.parse_args()

    # Read mtDNA sequence from text file
    if args.mtdna_seq_path is None:
        logger.info("Using default mtDNA sequence from resources/mito.txt")
        try:
            mtdna_seq = files('resources').joinpath('mito.txt').read_text().replace("\n", "")
        except FileNotFoundError:
            logger.error("Default mtDNA sequence file not found in resources/mito.txt")
            raise
    else:
        logger.info(f"Reading mtDNA sequence from file: {args.mtdna_seq_path}")
        with open(args.mtdna_seq_path, "r") as fh:
            mtdna_seq = fh.read().replace("\n", "")

    # Load additional bystander data if provided
    bystander_df = None
    if args.bystander_file:
        bystander_file = os.path.abspath(args.bystander_file)
        if os.path.isfile(bystander_file):
            logger.info(f"Loading bystander data from {bystander_file}")
            bystander_df = pd.read_excel(bystander_file)
        else:
            logger.warning(f"Bystander file {bystander_file} does not exist. Skipping bystander information.")

    # Prepare TALE-NT parameters
    tale_nt_params = {
        'min_spacer': args.min_spacer,
        'max_spacer': args.max_spacer,
        'array_min': args.array_min,
        'array_max': args.array_max,
        'filter': args.filter,
        'cut_pos': args.cut_pos
    }

    # Call the core processing function
    results = process_mitoedit(mtdna_seq=mtdna_seq,
                               position=args.position,
                               mutant_base=args.mutant_base,
                               bystander_df=bystander_df,
                               tale_nt_params=tale_nt_params)

    if results['windows_df'].empty:
        logger.warning("No results generated. Exiting.")
        return

    # Create output directory if it doesn't exist
    os.makedirs(args.output_prefix, exist_ok=True)
    logger.info(f"Output directory created/verified: {args.output_prefix}")

    # Write adjacent bases to FASTA file
    logger.info("Writing adjacent bases to FASTA file.")
    fasta_file = f'{args.output_prefix}/adjacent_bases.fasta'
    with open(fasta_file, 'w') as file:
        file.write(results['fasta_content'])
        logger.info(f"Finished writing FASTA file to {fasta_file}.")

    # Write pipeline CSV files
    pipeline_windows_csv = f'{args.output_prefix}/pipeline_windows.csv'
    pipeline_bystanders_csv = f'{args.output_prefix}/pipeline_bystanders.csv'

    logger.info(f"Writing pipeline windows data to {pipeline_windows_csv}.")
    results['windows_df'].to_csv(pipeline_windows_csv, index=False)

    if not results['bystanders_df'].empty:
        logger.info(f"Writing pipeline bystanders data to {pipeline_bystanders_csv}.")
        results['bystanders_df'].to_csv(pipeline_bystanders_csv, index=False)
    else:
        logger.info("No bystanders information available to write.")

    # Save results as combined files (same as individual since there's only one pipeline)
    combined_windows_csv = f'{args.output_prefix}/all_windows.csv'
    combined_bystanders_csv = f'{args.output_prefix}/all_bystanders.csv'

    logger.info(f"Saving combined windows data to {combined_windows_csv}.")
    results['windows_df'].to_csv(combined_windows_csv, index=False)

    if not results['bystanders_df'].empty:
        logger.info(f"Saving combined bystanders data to {combined_bystanders_csv}.")
        results['bystanders_df'].to_csv(combined_bystanders_csv, index=False)
    else:
        logger.info("No combined bystanders data to save.")

    # Save TALE-NT output if available
    if not results['talen_output_df'].empty:
        talen_output_path = f'{args.output_prefix}/talen_output.txt'
        logger.info(f"Saving TALE-NT output to {talen_output_path}.")
        results['talen_output_df'].to_csv(talen_output_path, sep='\t', index=False)

    logger.info("MitoEdit processing completed successfully.")


if __name__ == "__main__":
    main()
