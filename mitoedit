#!/usr/bin/env python

import argparse
import os
import re
from importlib.resources import files

import pandas as pd

from pipelines.Cho_sTALEDs import ChosTALEDsPipeline
from pipelines.Mok2020_unified import Mok2020UnifiedPipeline
from talent_tools.findTAL import RunFindTALTask
from talent_tools.talutil import OptionObject

import logging
import sys

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

logger = logging.getLogger('mitoedit')

# MAGIC NUMBERS for the TALE-NT Tool
MIN_SPACER = 14
MAX_SPACER = 18
ARR_MIN = 14
ARR_MAX = 18
FILTER = 1  # keep 1 for specific efficient TALES ELSE keep 2 to generate all kinds of TALE! (see TALEN FAQ page)
CUT_POS = 31


def main():
    parser = argparse.ArgumentParser(description='Process DNA sequence for base editing.')
    # yapf: disable
    parser.add_argument('--mtdna_seq_path', '-i', type=str, default=None,       help='File containing the mtDNA sequence as plain text.')
    parser.add_argument('--bystander_file'      , type=str,                     help='Excel file containing bystander effect annotations (optional, for human mtDNA analysis)')
    parser.add_argument('--output_prefix'       , type=str, default='output',   help='Prefix for output CSV files (default: output)')
    parser.add_argument('--min_spacer'          , type=int, default=MIN_SPACER, help=f'Minimum spacer length for TALE-NT (default: {MIN_SPACER})')
    parser.add_argument('--max_spacer'          , type=int, default=MAX_SPACER, help=f'Maximum spacer length for TALE-NT (default: {MAX_SPACER})')
    parser.add_argument('--array_min'           , type=int, default=ARR_MIN,    help=f'Minimum array length for TALE-NT (default: {ARR_MIN})')
    parser.add_argument('--array_max'           , type=int, default=ARR_MAX,    help=f'Maximum array length for TALE-NT (default: {ARR_MAX})')
    parser.add_argument('--filter'              , type=int, default=FILTER,     help=f'TALE-NT filter setting (default: {FILTER})')
    parser.add_argument('--cut_pos'             , type=int, default=CUT_POS,    help=f'TALE-NT cut position (default: {CUT_POS})')
    parser.add_argument('position'              , type=int,                     help='Position of the base to be changed')
    parser.add_argument('mutant_base'           , type=str,                     help='Mutant base to be changed into')
    # yapf: enable
    args = parser.parse_args()

    # Convert mutant base to uppercase
    mutant_base = args.mutant_base.upper()

    # Read mtDNA sequence from text file
    if args.mtdna_seq_path is None:
        logger.info("Using default mtDNA sequence from resources/mito.txt")
        try:
            mtdna_seq = files('resources').joinpath('mito.txt').read_text().replace("\n", "")
        except FileNotFoundError:
            logger.error("Default mtDNA sequence file not found in resources/mito.txt")
            raise
    else:
        logger.info(f"Reading mtDNA sequence from file: {args.mtdna_seq_path}")
        with open(args.mtdna_seq_path, "r") as fh:
            mtdna_seq = fh.read().replace("\n", "")

    # Get the reference base at the specified position
    reference_base = mtdna_seq[args.position - 1].upper()
    logger.info(f"Reference base at position {args.position} is {reference_base}")

    # Select appropriate pipeline based on base change type
    if (reference_base, mutant_base) in [('C', 'T'), ('G', 'A')]:
        pipeline_class = Mok2020UnifiedPipeline
        pipeline_name = "Mok2020_Unified"
    elif (reference_base, mutant_base) in [('A', 'G'), ('T', 'C')]:
        pipeline_class = ChosTALEDsPipeline
        pipeline_name = "Cho_sTALEDs"
    else:
        raise ValueError(f"No pipeline found for reference base {reference_base} and the mutant base {mutant_base}")

    logger.info(f"Selected pipeline: {pipeline_name}")

    # Create output directory if it doesn't exist
    os.makedirs(args.output_prefix, exist_ok=True)
    logger.info(f"Output directory created/verified: {args.output_prefix}")

    # Create the selected pipeline instance
    pipeline_instance = pipeline_class()

    logger.info(f"Processing mtDNA sequence for position {args.position}.")
    all_windows, adjacent_bases = pipeline_instance.process_mtDNA(mtdna_seq, args.position)

    if not adjacent_bases:
        logger.warning(f"The base found at position {args.position} cannot be edited.")
        return

    # Convert all_windows to DataFrame
    windows_df = pd.DataFrame(all_windows, columns=[
        'Pipeline', 'Position', 'Reference Base', 'Mutant Base', 'Window Size',
        'Window Sequence', 'Target Location', 'Number of Bystanders',
        'Position of Bystanders', 'Optimal Flanking TALEs', 'Flag (CheckBystanderEffect)'
    ])

    # Write adjacent bases to FASTA file
    logger.info("Writing adjacent bases to FASTA file.")
    fasta_content = f">Adjacent_bases_position_{args.position}\n{adjacent_bases}\n"
    fasta_file = f'{args.output_prefix}/adjacent_bases.fasta'
    with open(fasta_file, 'w') as file:
        file.write(fasta_content)
        logger.info(f"Finished writing FASTA file to {fasta_file}.")

    # Load additional bystander data if provided
    additional_df = None
    if args.bystander_file:
        additional_file = os.path.abspath(args.bystander_file)
        if os.path.isfile(additional_file):
            logger.info(f"Loading bystander data from {additional_file}")
            additional_df = pd.read_excel(additional_file)
        else:
            logger.warning(f"Bystander file {additional_file} does not exist. Skipping bystander information.")

    # Process pipeline data and get DataFrames
    logger.info("Processing pipeline data.")
    windows_df, bystanders_df = pipeline_instance.process_bystander_data(windows_df, additional_df)

    # Write pipeline CSV files
    pipeline_windows_csv = f'{args.output_prefix}/pipeline_windows.csv'
    pipeline_bystanders_csv = f'{args.output_prefix}/pipeline_bystanders.csv'

    logger.info(f"Writing pipeline windows data to {pipeline_windows_csv}.")
    windows_df.to_csv(pipeline_windows_csv, index=False)

    if not bystanders_df.empty:
        logger.info(f"Writing pipeline bystanders data to {pipeline_bystanders_csv}.")
        bystanders_df.to_csv(pipeline_bystanders_csv, index=False)
    else:
        logger.info("No bystanders information available to write.")

    # Save results as combined files (same as individual since there's only one pipeline)
    combined_windows_csv = f'{args.output_prefix}/all_windows.csv'
    combined_bystanders_csv = f'{args.output_prefix}/all_bystanders.csv'

    logger.info(f"Saving combined windows data to {combined_windows_csv}.")
    windows_df.to_csv(combined_windows_csv, index=False)

    if not bystanders_df.empty:
        logger.info(f"Saving combined bystanders data to {combined_bystanders_csv}.")
        bystanders_df.to_csv(combined_bystanders_csv, index=False)
    else:
        logger.info("No combined bystanders data to save.")

    logger.info("Pipeline processing completed successfully.")

    # Run TALE-NT analysis to find TALEN sequences
    talen_output_path = f'{args.output_prefix}/talen_output.txt'
    logger.info(f"Running TALE-NT findTAL analysis on {fasta_file}")

    RunFindTALTask(
        OptionObject(
            fasta=fasta_file,
            min=args.min_spacer,
            max=args.max_spacer,
            arraymin=args.array_min,
            arraymax=args.array_max,
            outpath=talen_output_path,
            filter=args.filter,
            filterbase=args.cut_pos if args.filter == 1 else -1,
            cupstream=0,
            gspec=False,
            streubel=False,
            check_offtargets=False,
            offtargets_fasta='NA',
            offtargets_ncbi='NA',
            genome=False,
            promoterome=False,
            organism='NA',
            logFilepath='NA',
            nodeID=-1,
            ip_address='',
        ))
    logger.info("TALE-NT analysis completed successfully")

    # Load and process TALEN output to match with pipeline results
    logger.info(f"Loading TXT file from {talen_output_path}")
    talen_output_df = pd.read_csv(talen_output_path, delimiter='\t', skiprows=2)
    logger.info("Successfully loaded TXT file.")

    if 'Plus strand sequence' not in talen_output_df.columns:
        raise ValueError("Column 'Plus strand sequence' not found in the TALEN file")

    plus_strand_sequences = talen_output_df['Plus strand sequence'].tolist()
    txt_bases = set(re.findall(r'[a-z]+', ' '.join(plus_strand_sequences)))

    # Match window sequences with TALEN sequences and extract left/right TALE parts
    # For each matching sequence, parse the TALEN format to separate left and right TALEs
    for index, row in windows_df.iterrows():
        cleaned_sequence = re.sub(r'[{}\[\]]', '', row['Window Sequence']).lower()
        windows_df.at[index, 'Matching TALEs'] = cleaned_sequence in txt_bases
        for txt_base in txt_bases:
            if txt_base == cleaned_sequence:
                left_index = 1
                right_index = 1
                matching_sequences = [
                    sequence for sequence in plus_strand_sequences if re.sub(r'[^a-z]', '', sequence) == txt_base
                ]
                for sequence in matching_sequences:
                    lower_indices = [i for i, char in enumerate(sequence) if char.islower()]
                    if len(lower_indices) >= 2:
                        spacer_start = lower_indices[0]
                        spacer_end = lower_indices[-1]
                        left_tale = sequence[:spacer_start].upper()
                        right_tale = sequence[spacer_end + 1:].upper()
                        windows_df.at[index, f'Left TALE {left_index}'] = left_tale
                        windows_df.at[index, f'Right TALE {right_index}'] = right_tale
                        left_index += 1
                        right_index += 1

    # Save updated results with TALEN matching information
    logger.info(f"Saving updated windows data with TALEN matching to {pipeline_windows_csv}.")
    windows_df.to_csv(pipeline_windows_csv, index=False)

    logger.info(f"Saving updated combined windows data with TALEN matching to {combined_windows_csv}.")
    windows_df.to_csv(combined_windows_csv, index=False)

    logger.info("All processing completed successfully.")


if __name__ == "__main__":
    main()
